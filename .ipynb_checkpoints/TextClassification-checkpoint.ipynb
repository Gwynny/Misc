{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разработка классификатора новостей\n",
    "\n",
    "**Нужно:**\n",
    "* выбрать какой-либо новостной ресурс, где к новостям привязаны категории или метки (например http://lenta.ru, http://fontanka.ru, http://gazeta.ru)\n",
    "* загрузить новости по некоторому набору (5-10) категорий за пару лет\n",
    "* обучить классификатор на эти новостях\n",
    "* продемонстрировать его работу, разработав простеший web-интерфейс (вариант - telegram-бот), куда пользователь вводит текст новости и на выходе получает наиболее вероятную категорию. В качестве фреймворка проще всего взять [Flask](http://flask.pocoo.org) (см. примеры) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['Технологии', 'ЖКХ', 'Происшествия', 'Власть', 'Общество', 'Строительство', 'Финансы',\n",
    "              'Бизнес', 'Туризм', 'Авто', 'Спорт', 'Гиды']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сформируем list со всеми линками на определенный день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for year in [2016,2017]:\n",
    "    for month in range(1,13):\n",
    "        for day in range(1,29):\n",
    "            if (day in range(1,10)) & (month in range(1,10)):\n",
    "                urls.append('http://www.fontanka.ru/fontanka/' + str(year) + '/0' \n",
    "                            + str(month) + '/0' + str(day) + '/all.html')\n",
    "            elif (day in range(1,10)) & (month in range(10,13)):\n",
    "                urls.append('http://www.fontanka.ru/fontanka/' + str(year) + '/'\n",
    "                                             + str(month) + '/0' + str(day) + '/all.html')\n",
    "            elif (month in range(1,10)) & (day in range(10,29)):\n",
    "                urls.append('http://www.fontanka.ru/fontanka/' + str(year) + '/0'\n",
    "                            + str(month) + '/' + str(day) + '/all.html')\n",
    "            else:\n",
    "                urls.append('http://www.fontanka.ru/fontanka/' + str(year) + '/'\n",
    "                            + str(month) + '/' + str(day) + '/all.html') \n",
    "def get_html(url):\n",
    "    req = Request(url)\n",
    "    webpage = urlopen(req)\n",
    "    return webpage.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для каждого дня спарсим все линки на новости этого дня, также спарсим тэг новости и заголовок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [04:42<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "headers = []\n",
    "links = []\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    soup = BeautifulSoup(get_html(url), \"html.parser\")\n",
    "    table = soup.find('div', attrs = {\"class\" : \"calendar-list\"})\n",
    "    try:\n",
    "        a = table.find_all('div', attrs = {\"class\" : \"calendar-item-title\"})\n",
    "        b = table.find_all('div', attrs = {\"calendar-item-category\"})\n",
    "    except:\n",
    "        continue\n",
    "    for i in range(len(a)):\n",
    "        if (b[i].a.text in categories) & (len(a[i].a['href']) == 16):\n",
    "            tags.append(b[i].a.text)\n",
    "            headers.append(a[i].a.text)\n",
    "            links.append(a[i].a['href'])\n",
    "    #time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Спарсим все эти новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61640/61640 [7:59:28<00:00,  2.14it/s]  \n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "errors = []\n",
    "#for i in tqdm(range(50)):\n",
    "for i in tqdm(range(len(links))):\n",
    "    url = 'http://www.fontanka.ru' + links[i]\n",
    "    try:\n",
    "        soup = BeautifulSoup(get_html(url), \"html.parser\")\n",
    "        table = soup.find('div', attrs = {\"class\" : \"article_fulltext\"})\n",
    "        list_ = []\n",
    "        string = ''\n",
    "        for j in range(len(table.find_all('p'))):\n",
    "            list_.append(table.find_all('p')[j].text.strip())\n",
    "            string = ' '.join(list_)\n",
    "        texts.append(''.join([headers[i], ' ', string]))\n",
    "    except:\n",
    "        errors.append(i)\n",
    "        texts.append('')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пока парсилось выключился wi-fi и некоторые новости не спарсились как надо, поэтому пройдемся заново по тем новостям, которые  вылетели с ошибкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:04<00:00, 10.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#texts = []\n",
    "errors1 = []\n",
    "#for i in tqdm(range(50)):\n",
    "for i in tqdm(errors):\n",
    "    url = 'http://www.fontanka.ru' + links[i]\n",
    "    try:\n",
    "        soup = BeautifulSoup(get_html(url), \"html.parser\")\n",
    "        table = soup.find('div', attrs = {\"class\" : \"article_fulltext\"})\n",
    "        list_ = []\n",
    "        string = ''\n",
    "        for j in range(len(table.find_all('p'))):\n",
    "            list_.append(table.find_all('p')[j].text.strip())\n",
    "            string = ' '.join(list_)\n",
    "        texts[i] = ''.join([headers[i], ' ', string])\n",
    "    except:\n",
    "        errors1.append(i)\n",
    "        texts[i] = ''\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраним всё, что получили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('errors.pickle', 'wb') as f:\n",
    "    pickle.dump(errors1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tags.pickle', 'wb') as f:\n",
    "    pickle.dump(tags, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Подготовка текста: токенизация -> лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import re\n",
    "import pickle\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "@lru_cache(maxsize=100000)\n",
    "def get_norm(word):\n",
    "    return morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('errors.pickle', 'rb') as f:\n",
    "    errors = pickle.load(f)\n",
    "    \n",
    "with open('data.pickle', 'rb') as f:\n",
    "    texts = pickle.load(f)\n",
    "    \n",
    "with open('tags.pickle', 'rb') as f:\n",
    "    tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61640\n",
      "61640\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Авто': 1234,\n",
       "         'Бизнес': 4624,\n",
       "         'Власть': 6661,\n",
       "         'Гиды': 3,\n",
       "         'ЖКХ': 807,\n",
       "         'Общество': 16121,\n",
       "         'Происшествия': 22183,\n",
       "         'Спорт': 5103,\n",
       "         'Строительство': 1276,\n",
       "         'Технологии': 764,\n",
       "         'Туризм': 674,\n",
       "         'Финансы': 2190})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалим новости, которые ничего не содержат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [e for e in texts if e not in '']\n",
    "for i in range(len(errors)):\n",
    "    if i > 0:\n",
    "        del tags[errors[i] - i]\n",
    "    else:\n",
    "        del tags[errors[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалим новости с тэгом \"Гиды\", так как их всего 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guides = np.where(np.array(tags) == 'Гиды')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(guides[0])):\n",
    "    if i > 0:\n",
    "        del tags[guides[0][i] - i]\n",
    "        del texts[guides[0][i] - i]\n",
    "    else:\n",
    "        del tags[guides[0][i]]\n",
    "        del texts[guides[0][i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Избавимся от лишних новостей с тэгами \"Происшествия\" и \"Общество\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prois = np.where(np.array(tags) == 'Происшествия')\n",
    "for i in range(15000):\n",
    "    if i > 0:\n",
    "        del tags[prois[0][i] - i]\n",
    "        del texts[prois[0][i] - i]\n",
    "    else:\n",
    "        del tags[prois[0][i]]\n",
    "        del texts[prois[0][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common = np.where(np.array(tags) == 'Общество')\n",
    "for i in range(10000):\n",
    "    if i > 0:\n",
    "        del tags[common[0][i] - i]\n",
    "        del texts[common[0][i] - i]\n",
    "    else:\n",
    "        del tags[common[0][i]]\n",
    "        del texts[common[0][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36622\n",
      "36622\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Авто': 1234,\n",
       "         'Бизнес': 4624,\n",
       "         'Власть': 6659,\n",
       "         'ЖКХ': 807,\n",
       "         'Общество': 6118,\n",
       "         'Происшествия': 7183,\n",
       "         'Спорт': 5102,\n",
       "         'Строительство': 1267,\n",
       "         'Технологии': 764,\n",
       "         'Туризм': 674,\n",
       "         'Финансы': 2190})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "for i in range(len(texts)):\n",
    "    words = re.findall('(?u)[А-Яа-яA-Za-z]+', texts[i])\n",
    "    tokenized_texts.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36622/36622 [01:38<00:00, 372.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm(range(len(tokenized_texts))):\n",
    "    helpful_list = []\n",
    "    for word in range(len(tokenized_texts[text])):\n",
    "        helpful_list.append(get_norm(tokenized_texts[text][word]))\n",
    "        if word == (len(tokenized_texts[text]) - 1):\n",
    "            tokenized_texts[text] = helpful_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(tokenized_texts)): \n",
    "    X.append(' '.join(tokenized_texts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с помощью логистической регрессии, можно и SVM, либо naive bayes, качество почти одинаковое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import  Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формируем tf-idf матрицу, также удаляем стоп слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('russian')\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 2, stop_words = stop_words)\n",
    "matrix_X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тут можно глянуть на качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "skf = StratifiedKFold(tags, n_folds = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labels1 = np.array(tags)\n",
    "# clf = LogisticRegression(C = 25, solver = 'newton-cg', multi_class = 'multinomial')\n",
    "# clf =  LinearSVC()\n",
    "# for train_index, test_index in skf:\n",
    "#     train_index = np.array(train_index)\n",
    "#     test_index = np.array(test_index)\n",
    "#     X_train, X_test = matrix_X[train_index], matrix_X[test_index]\n",
    "#     y_train, y_test = labels1[train_index], labels1[test_index]\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     preds_train = clf.predict(X_test)\n",
    "#     print(float(sum(preds_train == y_test)) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=25, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 25, solver = 'newton-cg', multi_class = 'multinomial')\n",
    "clf.fit(matrix_X, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказывать будем на основе вектора размерности (1 х количество слов в tf-idf матрице), то есть будем формировать вектор такой размерности и будем добавлять вычисленный tf-idf для входного слова в столбец, соответствующий этому слова в tf-idf матрице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финансы\n"
     ]
    }
   ],
   "source": [
    "word = 'доллар'\n",
    "if word in vectorizer.vocabulary_:\n",
    "    z = np.zeros((1,matrix_X.shape[1]))\n",
    "    z[0, vectorizer.vocabulary_[word]] = 0.7\n",
    "    print(clf.predict(z)[0])\n",
    "else:\n",
    "    print('Введите другое слово')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая версия предсказателя. Работает только для одного слова. Фиксированное значение tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_word(word):\n",
    "    if word in vectorizer.vocabulary_:\n",
    "        z = np.zeros((1,matrix_X.shape[1]))\n",
    "        z[0, vectorizer.vocabulary_[word]] = 0.7\n",
    "        return clf.predict(z)[0]\n",
    "    else:\n",
    "        return 'Введите другое слово'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая версия предсказателя. Работает для фраз. Считаем tf-idf для данного слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(words):\n",
    "    words = re.findall('(?u)[А-Яа-яA-Za-z]+', words)\n",
    "    z = np.zeros((1,matrix_X.shape[1]))\n",
    "    for word in words:\n",
    "        norm_word = get_norm(word)\n",
    "        if norm_word in vectorizer.vocabulary_:\n",
    "            idf = np.log(36622/len(np.where(np.array(matrix_X[:,vectorizer.vocabulary_[norm_word]].todense()) != 0)[0]))\n",
    "            tf = 1.0/len(words) #Будем считать, что слово встречается один раз в фразе\n",
    "            z[0, vectorizer.vocabulary_[norm_word]] = tf * idf\n",
    "                                                \n",
    "    if np.sum(z) != 0:\n",
    "        return clf.predict(z)[0]\n",
    "    else:\n",
    "        return 'Введите другое слово'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бот в 10 строк. Работать без включенного ноутбука, конечно, не будет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import telebot\n",
    "\n",
    "token = ''\n",
    "bot = telebot.TeleBot(token)\n",
    "\n",
    "@bot.message_handler(content_types=[\"text\"])\n",
    "\n",
    "\n",
    "def messages(message):\n",
    "    word = message.text\n",
    "    #modified_word = get_norm(word)\n",
    "    tag = check_word(word)#modified_word)\n",
    "    bot.send_message(message.chat.id, tag)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Черновик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
