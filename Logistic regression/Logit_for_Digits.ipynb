{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for logistic regression:\n",
      "learning rate for gradient descent = 0.008000\n",
      "lambda for regularization = 0.000500\n",
      "number of iterations = 100000\n",
      "\n",
      "Accuracy score for digits dataset = 0.962222222222\n",
      "Wall time: 47.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class MultiLogisticRegression():\n",
    "    \"\"\"\n",
    "    Logistic Regression for Multi-Class case based on Stanford lections\n",
    "    http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    theta : matrix(k, n)\n",
    "        all the parameters of our model\n",
    "        k - number of classes\n",
    "        n - number of features\n",
    "    learning_rate : float\n",
    "        learning_rate for SGD\n",
    "    lambda : float\n",
    "        lambda for weights regularization\n",
    "    n_iterations : int\n",
    "        number of iterations for SGD\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(x=inputs, y=targets)\n",
    "        Train model based on X, y\n",
    "    predict(x=inputs)\n",
    "        Train model based on X, y\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, theta,\n",
    "                 learning_rate=0.008,\n",
    "                 regularization_lambda=0.0005,\n",
    "                 n_iterations=100000):\n",
    "        self.theta = theta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_lambda = regularization_lambda\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def _standartize(self, x):\n",
    "        \"\"\"\n",
    "        Z-score Normalization for columns\n",
    "        #TODO Remove nested loop\n",
    "        \"\"\"\n",
    "\n",
    "        means, stds = np.mean(x, axis=0), np.std(x, axis=0)\n",
    "        stds[stds == 0] = 0.02\n",
    "        for i in range(x.shape[1]):\n",
    "            for j in range(x.shape[0]):\n",
    "                x[j, i] = (x[j, i] - means[i]) / stds[i]\n",
    "        return x\n",
    "\n",
    "    def _softmax(self, theta, x):\n",
    "        \"\"\"Softmax cost function\"\"\"\n",
    "\n",
    "        summed_softmaxes = 0\n",
    "        k = int(x.shape[0])\n",
    "        for i in range(len(self.theta)):\n",
    "            summed_softmaxes += np.exp(np.dot(self.theta[i], x.reshape((k, 1))))\n",
    "        z = np.exp(np.dot(theta, x.reshape((k, 1)))) / float(summed_softmaxes)\n",
    "        return z\n",
    "\n",
    "    def _indicator_function(self, y):\n",
    "        \"\"\"Indicator function for true label\"\"\"\n",
    "        indicator = [[1 if y[i] == k else 0 for k in range(len(np.unique(y)))] for i in range(len(y))]\n",
    "        return indicator\n",
    "\n",
    "    def _gradient(self, x, indicator):\n",
    "        \"\"\"Summed gradients for all objects\"\"\"\n",
    "        m = len(x)\n",
    "        actual_gradient = 0\n",
    "        summed_gradients_for_all_classes = []\n",
    "        for j in range(len(self.theta)):\n",
    "            random_ind = np.random.randint(x.shape[0])\n",
    "            actual_gradient += x[random_ind] * (indicator[random_ind][j] - self._softmax(self.theta[j], x[random_ind]))\n",
    "            summed_gradients_for_all_classes.append(self.learning_rate * ((-1 / m) * actual_gradient\n",
    "                                                                          + self.regularization_lambda * self.theta[j]))\n",
    "        return summed_gradients_for_all_classes\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Training process\"\"\"\n",
    "        x = self._standartize(x)\n",
    "        indicator = self._indicator_function(y)\n",
    "        for k in range(self.n_iterations):\n",
    "            self.theta = self.theta - self._gradient(x, indicator)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        Predicted class for inputted Test matrix\n",
    "        #TODO Remove nested loop\n",
    "        \"\"\"\n",
    "        x_test = self._standartize(x_test)\n",
    "        prob = np.ones((len(x_test), len(self.theta)))\n",
    "        for i in range(len(x_test)):\n",
    "            for j in range(len(self.theta)):\n",
    "                prob[i, j] = self._softmax(self.theta[j], x_test[i])\n",
    "        y_pred = []\n",
    "        for i in range(len(prob)):\n",
    "            y_pred.append(list(prob[i]).index(max(prob[i])))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def main():\n",
    "    # downloading digits dataset\n",
    "    data = load_digits()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "    # adding constant feature\n",
    "    z = np.ones((X_train.shape[0], 1))\n",
    "    X_train = np.hstack((X_train, z))\n",
    "    z = np.ones((X_test.shape[0], 1))\n",
    "    X_test = np.hstack((X_test, z))\n",
    "\n",
    "    # Creating parameters matrix\n",
    "    num_classes = len(np.unique(y))\n",
    "    theta = np.ones((num_classes, X_train.shape[1]))\n",
    "    \n",
    "    # Training\n",
    "    clf = MultiLogisticRegression(theta)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Parameters for logistic regression:\")\n",
    "    print(f\"learning rate for gradient descent = {clf.learning_rate}\")\n",
    "    print(f\"lambda for regularization = {clf.lamda}\")\n",
    "    print(f\"number of iterations = {clf.n_iterations}\\n\")\n",
    "    \n",
    "    print(\"Accuracy score for digits dataset =\", sum(y_pred == y_test) / float(len(y_pred)))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
